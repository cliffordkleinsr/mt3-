{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\cliffordkleinsr\\\\Documents\\\\Clido_Projects\\\\mt3\\\\amt\\\\src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\model\\RoPE\\RoPE.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "c:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\model\\RoPE\\RoPE.py:242: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "c:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from ../logs\\2024\\ptf_all_cross_rebal5_mirst_xk2_edr005_attend_c_full_plus_b100\\checkpoints\\model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: mt3_full_plus, Max Shift Steps: 206\n",
      "\"add_melody_metric_to_singing\": True\n",
      "\"add_pitch_class_metric\":       None\n",
      "\"audio_cfg\":                    {'codec': 'spec', 'hop_length': 300, 'audio_backend': 'torchaudio', 'sample_rate': 16000, 'input_frames': 32767, 'n_fft': 2048, 'n_mels': 512, 'f_min': 50.0, 'f_max': 8000.0}\n",
      "\"base_lr\":                      None\n",
      "\"eval_drum_vocab\":              None\n",
      "\"eval_subtask_key\":             default\n",
      "\"eval_vocab\":                   None\n",
      "\"init_factor\":                  None\n",
      "\"max_steps\":                    None\n",
      "\"model_cfg\":                    {'encoder_type': 'perceiver-tf', 'decoder_type': 't5', 'pre_encoder_type': 'conv', 'pre_encoder_type_default': {'t5': None, 'perceiver-tf': 'conv', 'conformer': None}, 'pre_decoder_type': 'linear', 'pre_decoder_type_default': {'t5': {'t5': None}, 'perceiver-tf': {'t5': 'linear', 'multi-t5': 'mc_shared_linear'}, 'conformer': {'t5': None}}, 'conv_out_channels': 128, 't5_basename': 'google/t5-v1_1-small', 'pretrained': False, 'use_task_conditional_encoder': True, 'use_task_conditional_decoder': True, 'd_feat': 128, 'tie_word_embeddings': True, 'vocab_size': 596, 'num_max_positions': 1034, 'encoder': {'t5': {'d_model': 512, 'num_heads': 6, 'num_layers': 8, 'dropout_rate': 0.05, 'position_encoding_type': 'sinusoidal', 'ff_widening_factor': 2, 'ff_layer_type': 't5_gmlp'}, 'perceiver-tf': {'num_latents': 24, 'd_latent': 128, 'd_model': 128, 'num_blocks': 3, 'num_local_transformers_per_block': 2, 'num_temporal_transformers_per_block': 2, 'sca_use_query_residual': False, 'dropout_rate': 0.1, 'position_encoding_type': 'trainable', 'attention_to_channel': True, 'layer_norm_type': 'layer_norm', 'ff_layer_type': 'mlp', 'ff_widening_factor': 1, 'moe_num_experts': 4, 'moe_topk': 2, 'hidden_act': 'gelu', 'rotary_type_sca': 'pixel', 'rotary_type_latent': 'pixel', 'rotary_type_temporal': 'lang', 'rotary_apply_to_keys': False, 'rotary_partial_pe': False, 'num_max_positions': 110, 'vocab_size': 596}, 'conformer': {'d_model': 512, 'intermediate_size': 512, 'num_heads': 8, 'num_layers': 8, 'dropout_rate': 0.1, 'layerdrop': 0.1, 'position_encoding_type': 'rotary', 'conv_dim': (512, 512, 512, 512, 512, 512, 512), 'conv_stride': (5, 2, 2, 2, 2, 2, 2), 'conv_kernel': (10, 3, 3, 3, 3, 3, 3), 'conv_depthwise_kernel_size': 31}}, 'decoder': {'t5': {'d_model': 512, 'num_heads': 6, 'num_layers': 8, 'dropout_rate': 0.05, 'position_encoding_type': 'sinusoidal', 'ff_widening_factor': 2, 'ff_layer_type': 't5_gmlp', 'num_max_positions': 1034, 'vocab_size': 596}, 'multi-t5': {'d_model': 512, 'num_heads': 6, 'num_layers': 8, 'dropout_rate': 0.05, 'position_encoding_type': 'sinusoidal', 'ff_widening_factor': 2, 'ff_layer_type': 't5_gmlp', 'num_channels': 13}}, 'feat_length': 110, 'event_length': 1024, 'init_factor': 1.0}\n",
      "\"onset_tolerance\":              0.05\n",
      "\"optimizer\":                    None\n",
      "\"optimizer_name\":               adamwscale\n",
      "\"pretrained\":                   False\n",
      "\"scheduler_name\":               cosine\n",
      "\"shared_cfg\":                   {'PATH': {'data_home': '../../data'}, 'BSZ': {'train_sub': 12, 'train_local': 24, 'validation': 64, 'test': 64}, 'AUGMENTATION': {'train_random_amp_range': [0.8, 1.1], 'train_stem_iaug_prob': 0.7, 'train_stem_xaug_policy': {'max_k': 3, 'tau': 0.3, 'alpha': 1.0, 'max_subunit_stems': 12, 'p_include_singing': None, 'no_instr_overlap': True, 'no_drum_overlap': True, 'uhat_intra_stem_augment': True}, 'train_pitch_shift_range': [-2, 2]}, 'DATAIO': {'num_workers': 4, 'prefetch_factor': 2, 'pin_memory': True, 'persistent_workers': False}, 'CHECKPOINT': {'save_top_k': 4, 'monitor': 'validation/macro_onset_f', 'mode': 'max', 'save_last': True, 'filename': '{epoch}-{step}'}, 'TRAINER': {'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'gradient_clip_val': 1.0, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1, 'num_sanity_val_steps': 0}, 'WANDB': {'save_dir': '../logs', 'resume': 'allow', 'anonymous': 'allow', 'mode': 'disabled'}, 'LR_SCHEDULE': {'warmup_steps': 1000, 'total_steps': 100000, 'final_cosine': 1e-05}, 'TOKENIZER': {'max_shift_steps': 206, 'shift_step_ms': 10}}\n",
      "\"task_manager\":                 <utils.task_manager.TaskManager object at 0x000001F67300A350>\n",
      "\"test_optimal_octave_shift\":    False\n",
      "\"test_pitch_shift_layer\":       None\n",
      "\"weight_decay\":                 0.0\n",
      "\"write_output_dir\":             ../logs\\2024\\ptf_all_cross_rebal5_mirst_xk2_edr005_attend_c_full_plus_b100\n",
      "\"write_output_vocab\":           None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\inference_utils.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(dir_info[\"last_ckpt_path\"])\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import load_model_checkpoint, transcribe\n",
    "\n",
    "model_name = 'YPTF+Single (noPS)'\n",
    "precision = '16'\n",
    "project = '2024'\n",
    "\n",
    "\n",
    "match model_name:\n",
    "    case \"YMT3+\":\n",
    "        checkpoint = \"notask_all_cross_v6_xk2_amp0811_gm_ext_plus_nops_b72@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-pr', precision]\n",
    "    case \"YPTF+Single (noPS)\":\n",
    "        checkpoint = \"ptf_all_cross_rebal5_mirst_xk2_edr005_attend_c_full_plus_b100@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-enc', 'perceiver-tf', '-ac', 'spec',\n",
    "                '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case \"YPTF+Multi (PS)\":\n",
    "        checkpoint = \"mc13_256_all_cross_v6_xk5_amp0811_edr005_attend_c_full_plus_2psn_nl26_sb_b26r_800k@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-tk', 'mc13_full_plus_256',\n",
    "                '-dec', 'multi-t5', '-nl', '26', '-enc', 'perceiver-tf',\n",
    "                '-ac', 'spec', '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case \"YPTF.MoE+Multi (noPS)\":\n",
    "        checkpoint = \"mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b36_nops@last.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-tk', 'mc13_full_plus_256', '-dec', 'multi-t5',\n",
    "                '-nl', '26', '-enc', 'perceiver-tf', '-sqr', '1', '-ff', 'moe',\n",
    "                '-wf', '4', '-nmoe', '8', '-kmoe', '2', '-act', 'silu', '-epe', 'rope',\n",
    "                '-rp', '1', '-ac', 'spec', '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case \"YPTF.MoE+Multi (PS)\":\n",
    "        checkpoint = \"mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b80_ps2@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-tk', 'mc13_full_plus_256', '-dec', 'multi-t5',\n",
    "                '-nl', '26', '-enc', 'perceiver-tf', '-sqr', '1', '-ff', 'moe',\n",
    "                '-wf', '4', '-nmoe', '8', '-kmoe', '2', '-act', 'silu', '-epe', 'rope',\n",
    "                '-rp', '1', '-ac', 'spec', '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case _:\n",
    "        raise ValueError(model_name)\n",
    "        \n",
    "model = load_model_checkpoint(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ converting audio: 0m 0s 259.18ms\n",
      "⏰ model inference: 0m 47s 285.12ms\n",
      "⏰ model inference complete: 0m 0s 75.28ms\n"
     ]
    }
   ],
   "source": [
    "audio_file='Autumn  Leaves  - Jazz Trio.wav'\n",
    "preds = transcribe(model=model, audio_file=audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.note_event_dataclasses import Note\n",
    "# from utils.note2event import note2note_event\n",
    "# from utils.midi import note_event2midi\n",
    "# import os\n",
    "# def write_model_output_as_midi(notes, output_dir, track_name, output_inverse_vocab):\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     output_file = os.path.join(output_dir, f\"{track_name}.mid\")\n",
    "#     if output_inverse_vocab is not None:\n",
    "#         # Convert the note events to the output vocabulary\n",
    "#         new_notes = []\n",
    "#         for note in notes:\n",
    "#             if note.is_drum:\n",
    "#                 new_notes.append(note)\n",
    "#             else:\n",
    "#                 new_notes.append(\n",
    "#                     Note(is_drum=note.is_drum,\n",
    "#                          program=output_inverse_vocab.get(note.program, [note.program])[0],\n",
    "#                          onset=note.onset,\n",
    "#                          offset=note.offset,\n",
    "#                          pitch=note.pitch,\n",
    "#                          velocity=note.velocity))\n",
    "\n",
    "#     note_events = note2note_event(new_notes, return_activity=False)\n",
    "#     note_event2midi(note_events, output_file, output_inverse_vocab=output_inverse_vocab)\n",
    "#     print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio\\\\Autumn  Leaves  - Jazz Trio.mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m track_name:\u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m audio_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Write MIDI\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m write_model_output_as_midi(preds, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m, track_name, model\u001b[38;5;241m.\u001b[39mmidi_output_inverse_vocab)\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\utils\\utils.py:177\u001b[0m, in \u001b[0;36mwrite_model_output_as_midi\u001b[1;34m(notes, output_dir, track_name, output_inverse_vocab)\u001b[0m\n\u001b[0;32m    168\u001b[0m             new_notes\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    169\u001b[0m                 Note(is_drum\u001b[38;5;241m=\u001b[39mnote\u001b[38;5;241m.\u001b[39mis_drum,\n\u001b[0;32m    170\u001b[0m                      program\u001b[38;5;241m=\u001b[39moutput_inverse_vocab\u001b[38;5;241m.\u001b[39mget(note\u001b[38;5;241m.\u001b[39mprogram, [note\u001b[38;5;241m.\u001b[39mprogram])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m                      pitch\u001b[38;5;241m=\u001b[39mnote\u001b[38;5;241m.\u001b[39mpitch,\n\u001b[0;32m    174\u001b[0m                      velocity\u001b[38;5;241m=\u001b[39mnote\u001b[38;5;241m.\u001b[39mvelocity))\n\u001b[0;32m    176\u001b[0m note_events \u001b[38;5;241m=\u001b[39m note2note_event(new_notes, return_activity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 177\u001b[0m note_event2midi(note_events, output_file, output_inverse_vocab\u001b[38;5;241m=\u001b[39moutput_inverse_vocab)\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\utils\\midi.py:340\u001b[0m, in \u001b[0;36mnote_event2midi\u001b[1;34m(note_events, output_file, velocity, ticks_per_beat, tempo, singing_program_mapping, singing_chorus_program_mapping, output_inverse_vocab)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Save MIDI file\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     midi\u001b[38;5;241m.\u001b[39msave(output_file)\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\mido\\midifiles\\midifiles.py:457\u001b[0m, in \u001b[0;36mMidiFile.save\u001b[1;34m(self, filename, file)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(file)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(file)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio\\\\Autumn  Leaves  - Jazz Trio.mid'"
     ]
    }
   ],
   "source": [
    "from utils.utils import write_model_output_as_midi\n",
    "track_name:str = audio_file.split('.wav')[0]\n",
    "\n",
    "# Write MIDI\n",
    "write_model_output_as_midi(preds, 'audio', track_name, model.midi_output_inverse_vocab)\n",
    "# midifile =  os.path.join('audio', track_name  + '.mid')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
