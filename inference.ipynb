{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\cliffordkleinsr\\\\Documents\\\\Clido_Projects\\\\mt3\\\\amt\\\\src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\model\\RoPE\\RoPE.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "c:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\model\\RoPE\\RoPE.py:242: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "c:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from ../logs\\2024\\notask_all_cross_v6_xk2_amp0811_gm_ext_plus_nops_b72\\checkpoints\\model.ckpt\n",
      "Task: mt3_full_plus, Max Shift Steps: 206\n",
      "\"add_melody_metric_to_singing\": True\n",
      "\"add_pitch_class_metric\":       None\n",
      "\"audio_cfg\":                    {'codec': 'melspec', 'hop_length': 128, 'audio_backend': 'torchaudio', 'sample_rate': 16000, 'input_frames': 32767, 'n_fft': 2048, 'n_mels': 512, 'f_min': 50.0, 'f_max': 8000.0}\n",
      "\"base_lr\":                      None\n",
      "\"eval_drum_vocab\":              None\n",
      "\"eval_subtask_key\":             default\n",
      "\"eval_vocab\":                   None\n",
      "\"init_factor\":                  None\n",
      "\"max_steps\":                    None\n",
      "\"model_cfg\":                    {'encoder_type': 't5', 'decoder_type': 't5', 'pre_encoder_type': None, 'pre_encoder_type_default': {'t5': None, 'perceiver-tf': 'conv', 'conformer': None}, 'pre_decoder_type': None, 'pre_decoder_type_default': {'t5': {'t5': None}, 'perceiver-tf': {'t5': 'linear', 'multi-t5': 'mc_shared_linear'}, 'conformer': {'t5': None}}, 'conv_out_channels': 128, 't5_basename': 'google/t5-v1_1-small', 'pretrained': False, 'use_task_conditional_encoder': True, 'use_task_conditional_decoder': True, 'd_feat': 512, 'tie_word_embeddings': True, 'vocab_size': 596, 'num_max_positions': 1034, 'encoder': {'t5': {'d_model': 512, 'num_heads': 6, 'num_layers': 8, 'dropout_rate': 0.05, 'position_encoding_type': 'sinusoidal', 'ff_widening_factor': 2, 'ff_layer_type': 't5_gmlp', 'num_max_positions': 1034, 'vocab_size': 596}, 'perceiver-tf': {'num_latents': 24, 'd_latent': 128, 'd_model': 'q', 'num_blocks': 3, 'num_local_transformers_per_block': 2, 'num_temporal_transformers_per_block': 2, 'sca_use_query_residual': False, 'dropout_rate': 0.1, 'position_encoding_type': 'trainable', 'attention_to_channel': True, 'layer_norm_type': 'layer_norm', 'ff_layer_type': 'mlp', 'ff_widening_factor': 1, 'moe_num_experts': 4, 'moe_topk': 2, 'hidden_act': 'gelu', 'rotary_type_sca': 'pixel', 'rotary_type_latent': 'pixel', 'rotary_type_temporal': 'lang', 'rotary_apply_to_keys': False, 'rotary_partial_pe': False}, 'conformer': {'d_model': 512, 'intermediate_size': 512, 'num_heads': 8, 'num_layers': 8, 'dropout_rate': 0.1, 'layerdrop': 0.1, 'position_encoding_type': 'rotary', 'conv_dim': (512, 512, 512, 512, 512, 512, 512), 'conv_stride': (5, 2, 2, 2, 2, 2, 2), 'conv_kernel': (10, 3, 3, 3, 3, 3, 3), 'conv_depthwise_kernel_size': 31}}, 'decoder': {'t5': {'d_model': 512, 'num_heads': 6, 'num_layers': 8, 'dropout_rate': 0.05, 'position_encoding_type': 'sinusoidal', 'ff_widening_factor': 2, 'ff_layer_type': 't5_gmlp', 'num_max_positions': 1034, 'vocab_size': 596}, 'multi-t5': {'d_model': 512, 'num_heads': 6, 'num_layers': 8, 'dropout_rate': 0.05, 'position_encoding_type': 'sinusoidal', 'ff_widening_factor': 2, 'ff_layer_type': 't5_gmlp', 'num_channels': 13}}, 'feat_length': 256, 'event_length': 1024, 'init_factor': 1.0}\n",
      "\"onset_tolerance\":              0.05\n",
      "\"optimizer\":                    None\n",
      "\"optimizer_name\":               adamwscale\n",
      "\"pretrained\":                   False\n",
      "\"scheduler_name\":               cosine\n",
      "\"shared_cfg\":                   {'PATH': {'data_home': '../../data'}, 'BSZ': {'train_sub': 12, 'train_local': 24, 'validation': 64, 'test': 64}, 'AUGMENTATION': {'train_random_amp_range': [0.8, 1.1], 'train_stem_iaug_prob': 0.7, 'train_stem_xaug_policy': {'max_k': 3, 'tau': 0.3, 'alpha': 1.0, 'max_subunit_stems': 12, 'p_include_singing': None, 'no_instr_overlap': True, 'no_drum_overlap': True, 'uhat_intra_stem_augment': True}, 'train_pitch_shift_range': [-2, 2]}, 'DATAIO': {'num_workers': 4, 'prefetch_factor': 2, 'pin_memory': True, 'persistent_workers': False}, 'CHECKPOINT': {'save_top_k': 4, 'monitor': 'validation/macro_onset_f', 'mode': 'max', 'save_last': True, 'filename': '{epoch}-{step}'}, 'TRAINER': {'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'gradient_clip_val': 1.0, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1, 'num_sanity_val_steps': 0}, 'WANDB': {'save_dir': '../logs', 'resume': 'allow', 'anonymous': 'allow', 'mode': 'disabled'}, 'LR_SCHEDULE': {'warmup_steps': 1000, 'total_steps': 100000, 'final_cosine': 1e-05}, 'TOKENIZER': {'max_shift_steps': 206, 'shift_step_ms': 10}}\n",
      "\"task_manager\":                 <utils.task_manager.TaskManager object at 0x000001608AB9E690>\n",
      "\"test_optimal_octave_shift\":    False\n",
      "\"test_pitch_shift_layer\":       None\n",
      "\"weight_decay\":                 0.0\n",
      "\"write_output_dir\":             ../logs\\2024\\notask_all_cross_v6_xk2_amp0811_gm_ext_plus_nops_b72\n",
      "\"write_output_vocab\":           None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cliffordkleinsr\\Documents\\Clido_Projects\\mt3\\amt\\src\\inference_utils.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(dir_info[\"last_ckpt_path\"])\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import load_model_checkpoint, transcribe\n",
    "\n",
    "model_name = \"YMT3+\" #'YPTF+Single (noPS)'\n",
    "precision = '16'\n",
    "project = '2024'\n",
    "\n",
    "\n",
    "match model_name:\n",
    "    case \"YMT3+\":\n",
    "        checkpoint = \"notask_all_cross_v6_xk2_amp0811_gm_ext_plus_nops_b72@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-pr', precision]\n",
    "    case \"YPTF+Single (noPS)\":\n",
    "        checkpoint = \"ptf_all_cross_rebal5_mirst_xk2_edr005_attend_c_full_plus_b100@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-enc', 'perceiver-tf', '-ac', 'spec',\n",
    "                '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case \"YPTF+Multi (PS)\":\n",
    "        checkpoint = \"mc13_256_all_cross_v6_xk5_amp0811_edr005_attend_c_full_plus_2psn_nl26_sb_b26r_800k@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-tk', 'mc13_full_plus_256',\n",
    "                '-dec', 'multi-t5', '-nl', '26', '-enc', 'perceiver-tf',\n",
    "                '-ac', 'spec', '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case \"YPTF.MoE+Multi (noPS)\":\n",
    "        checkpoint = \"mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b36_nops@last.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-tk', 'mc13_full_plus_256', '-dec', 'multi-t5',\n",
    "                '-nl', '26', '-enc', 'perceiver-tf', '-sqr', '1', '-ff', 'moe',\n",
    "                '-wf', '4', '-nmoe', '8', '-kmoe', '2', '-act', 'silu', '-epe', 'rope',\n",
    "                '-rp', '1', '-ac', 'spec', '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case \"YPTF.MoE+Multi (PS)\":\n",
    "        checkpoint = \"mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b80_ps2@model.ckpt\"\n",
    "        args = [checkpoint, '-p', project, '-tk', 'mc13_full_plus_256', '-dec', 'multi-t5',\n",
    "                '-nl', '26', '-enc', 'perceiver-tf', '-sqr', '1', '-ff', 'moe',\n",
    "                '-wf', '4', '-nmoe', '8', '-kmoe', '2', '-act', 'silu', '-epe', 'rope',\n",
    "                '-rp', '1', '-ac', 'spec', '-hop', '300', '-atc', '1', '-pr', precision]\n",
    "    case _:\n",
    "        raise ValueError(model_name)\n",
    "        \n",
    "model = load_model_checkpoint(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ converting audio: 0m 0s 469.36ms\n",
      "⏰ model inference: 1m 1s 168.15ms\n",
      "⏰ model inference complete: 0m 0s 291.38ms\n"
     ]
    }
   ],
   "source": [
    "audio_file='08 8  A Foggy Day.wav'\n",
    "preds = transcribe(model=model, audio_file=audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import write_model_output_as_midi\n",
    "track_name:str = audio_file.split('.wav')[0]\n",
    "\n",
    "# Write MIDI\n",
    "write_model_output_as_midi(preds, 'audio', track_name, model.midi_output_inverse_vocab)\n",
    "# midifile =  os.path.join('audio', track_name  + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m6\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m b_compiled \u001b[38;5;241m=\u001b[39m add(a, b)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(b_compiled)\n",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m, in \u001b[0;36madd\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     19\u001b[0m n_elements \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m     20\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m meta: (triton\u001b[38;5;241m.\u001b[39mcdiv(n_elements, meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOCK_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m]),)\n\u001b[1;32m---> 21\u001b[0m add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\triton\\runtime\\jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(grid\u001b[38;5;241m=\u001b[39mgrid, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\triton\\runtime\\jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[1;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    663\u001b[0m         src,\n\u001b[0;32m    664\u001b[0m         target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[0;32m    665\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[0;32m    666\u001b[0m     )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\triton\\compiler\\compiler.py:244\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(src, target, options)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# create cache manager\u001b[39;00m\n\u001b[0;32m    243\u001b[0m env_vars \u001b[38;5;241m=\u001b[39m get_cache_invalidating_env_vars()\n\u001b[1;32m--> 244\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriton_key()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msorted\u001b[39m(env_vars\u001b[38;5;241m.\u001b[39mitems()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha256(key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[0;32m    246\u001b[0m fn_cache_manager \u001b[38;5;241m=\u001b[39m get_cache_manager(\u001b[38;5;28mhash\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\triton\\backends\\nvidia\\compiler.py:336\u001b[0m, in \u001b[0;36mCUDABackend.hash\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache()\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 336\u001b[0m     version \u001b[38;5;241m=\u001b[39m get_ptxas_version()\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\triton\\backends\\nvidia\\compiler.py:38\u001b[0m, in \u001b[0;36mget_ptxas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ptxas_version\u001b[39m():\n\u001b[1;32m---> 38\u001b[0m     version \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output([_path_to_binary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptxas\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--version\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "File \u001b[1;32mc:\\Users\\cliffordkleinsr\\anaconda3\\envs\\mt3\\Lib\\site-packages\\triton\\backends\\nvidia\\compiler.py:23\u001b[0m, in \u001b[0;36m_path_to_binary\u001b[1;34m(binary)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     19\u001b[0m     binary \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     21\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRITON_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbinary\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     22\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m\"\u001b[39m, binary),\n\u001b[1;32m---> 23\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m\"\u001b[39m, binary),\n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mbin\u001b[39m \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mbin\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mbin\u001b[39m):\n",
      "File \u001b[1;32m<frozen ntpath>:108\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "a = torch.rand(3, device=\"cuda\")\n",
    "b = torch.rand(6, device=\"cuda\")\n",
    "b_compiled = add(a, b)\n",
    "print(b_compiled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
